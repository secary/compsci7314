{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop week 6\n",
    "\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm used for both classification and regression tasks. It belongs to the family of supervised learning algorithms and is particularly effective in high-dimensional spaces. SVMs are widely used in various fields, including image recognition, text classification, and bioinformatics.\n",
    "\n",
    "Here are some key concepts associated with Support Vector Machines:\n",
    "\n",
    "### Objective:\n",
    "In a classification task, the primary goal of an SVM is to find a hyperplane that best separates the data points belonging to different classes. This hyperplane is chosen to maximize the margin between the classes, which is the distance between the hyperplane and the nearest data point of each class.\n",
    "\n",
    "### Hyperplane:\n",
    "In a two-dimensional space, a hyperplane is a simple line. However, in higher dimensions, it becomes a hyperplane. For example, in a three-dimensional space, it would be a plane, and in more dimensions, it would be a hyperplane.\n",
    "\n",
    "### Support Vectors:\n",
    "Support Vectors are the data points that lie closest to the decision boundary (hyperplane) and have the most significant influence on determining the optimal hyperplane. These are the critical elements in SVM as they define and support the decision boundary.\n",
    "\n",
    "### Margin:\n",
    "The margin is the gap between the decision boundary and the nearest data point from each class. A wider margin often corresponds to a more robust and generalized model, reducing the risk of overfitting.\n",
    "\n",
    "### Kernel Trick:\n",
    "SVMs can efficiently handle non-linear decision boundaries by employing the kernel trick. Kernels allow the algorithm to implicitly map the input space into a higher-dimensional space, making it possible to find a linear decision boundary in the transformed feature space.\n",
    "\n",
    "### C Parameter:\n",
    "The C parameter in SVM is a regularization term that balances the goal of achieving a wide margin and correctly classifying training data points. A smaller C allows a more robust margin but may misclassify some training data, while a larger C aims to classify all training data correctly, potentially leading to a narrower margin.\n",
    "\n",
    "Support Vector Machines are known for their versatility, robustness, and effectiveness in various real-world applications. They excel in scenarios where the data is not linearly separable or has complex patterns. SVMs are a valuable tool in the machine learning toolkit for tasks ranging from text classification to image recognition and beyond.\n",
    "\n",
    "\n",
    "The primal and dual formulations in Support Vector Machines (SVMs) are two ways of expressing the same optimization problem. The primal formulation directly seeks the optimal weights and bias for the hyperplane in the input space, while the dual formulation involves finding the weights of the support vectors using Lagrange multipliers. The dual formulation is advantageous in certain cases, especially when dealing with high-dimensional data or applying the kernel trick for non-linear classification. Both formulations lead to the same optimal solution, showcasing the duality relationship in SVMs. The choice between primal and dual depends on computational efficiency and the characteristics of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the primal form of SVM (with hard margin), we aim to minimize the objective function while subject to constraints. \n",
    "\n",
    "### The objective function is:\n",
    "\n",
    "**Minimize (w, b):**\n",
    "$$\n",
    "\\frac{1}{2} \\|\\mathbf{w}\\|^2 \n",
    "$$\n",
    "\n",
    "This objective function represents the margin maximization problem. We want to find the values of $w$ and $b$ that minimize $\\frac{1}{2} \\|\\mathbf{w}\\|^2$ while satisfying the constraints.\n",
    "\n",
    "Constraints:\n",
    "\n",
    "The constraints ensure that data points are correctly classified and are on or beyond the margin boundary. For each training sample $(x_{i}, y_{i})$ where $x_{i}$ is the feature vector and $y_{i}$ is the class label, the following constraint is enforced:\n",
    "\n",
    "$$y_{i}(w^Tx_{i} + b) ≥ 1 $$\n",
    "\n",
    "This constraint ensures that data points are correctly classified by the hyperplane and are positioned beyond or on the margin.\n",
    "\n",
    "**Lagrange Multipliers $(α_i)$:**\n",
    "\n",
    "To solve the optimization problem, we use Lagrange multipliers $(α_i, \\beta_i) $ for each constraint. The Langrangian function is defined as: \n",
    "\n",
    "$$ min_{w,  \\xi, b} max_{α, \\beta, w,  \\xi_i, b} (L(w,b,α,\\beta,\\xi))= min_{w,  \\xi, b}(\\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C \\sum_i \\xi_i) + max_{α, \\beta, w,  \\xi_i, b}( \\sum_i \\alpha_i \\left(1 - \\xi_i - y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right) - \\sum_i \\beta_i \\xi_i)$$\n",
    "\n",
    "\n",
    "Equivalent form of the above equation used to find Lagrange multipliers analytically is\n",
    "\n",
    "\n",
    "$$\n",
    "max_{\\alpha_i, \\alpha_j}(\\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C\n",
    "$$\n",
    "$$\n",
    "\\sum_i \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "$α,\\beta$ are vectors of Lagrange multipliers,one for each instance.\n",
    "\n",
    "$m$ is the number of training samples.\n",
    "\n",
    "$\\xi$ are slack variables used for soft margin SVM\n",
    "C is the hyperparameter determining how much slack should be given to training examples to optimise the cost function\n",
    "\n",
    "The Lagrangian function combines the objective function with the constraints, weighted by the Lagrange multipliers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are tasked with building an SVM classifier to separate two classes (Class A and Class B) in a two-dimensional feature space. The data points in Class A are represented as $(x_1, x_2)$ and the data points in Class B are represented as $(y_1, y_2)$. Your goal is to find the optimal hyperplane that maximizes the margin between the two classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Find SVM parameters using SVC\n",
    "\n",
    "Use the code template for this task\n",
    "\n",
    "1) Determine the parameters of the hyperplane.\n",
    "2) Calculate the margin and identify the support vectors.\n",
    "3) Change C parameter in SVC from small (e.g. 0.01) to large (e.g. 10) and observer how the support vectors and margin changes.\n",
    "4) On a piece of paper, draw a diagram showing how the margin changes vs C value. Explain the plot. Is the plot linear? Why or why not?\n",
    "\n",
    "Implement SVM classifier to classify given instances. Then find cupport vectors and margin. <br>\n",
    "Use SVC documentation to find required APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate a small dataset\n",
    "np.random.seed(41)  # For reproducibility\n",
    "\n",
    "# 10 positive points around (2, 2)\n",
    "class_a = np.random.randn(10, 2) + [1, 1.5]\n",
    "\n",
    "# 10 negative points around (-2, -2)\n",
    "class_b = np.random.randn(10, 2) + [-1, -1.5]\n",
    "\n",
    "# Combine the data\n",
    "X = np.vstack((class_a, class_b))\n",
    "y = np.array([1] * 10 + [-1] * 10)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel and fit the model\n",
    "\n",
    "\n",
    "# Obtain the support vectors, weights, bias, and calculate the margin\n",
    "\n",
    "\n",
    "# Print the support vectors and margin\n",
    "print(\"Support Vectors:\")\n",
    "print(support_vectors)\n",
    "\n",
    "print(\"Margin:\")\n",
    "print(margin)\n",
    "\n",
    "print(\"weights:\")\n",
    "print(weights)\n",
    "\n",
    "print(\"bias:\")\n",
    "print(bias)\n",
    "\n",
    "\n",
    "# Plot the decision boundary, margin boundaries, and support vectors\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),\n",
    "                     np.linspace(ylim[0], ylim[1], 100))\n",
    "Z = svm_classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "            linestyles=['--', '-', '--'])\n",
    "\n",
    "# Highlight support vectors\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100,\n",
    "            linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"SVM Decision Boundary, Margin Boundaries, and Support Vectors\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we used the coef_ attribute to access the weight vector w and the intercept attribute to access the bias b of the hyperplane. These values are determined by the SVM model based on the support vectors and Lagrange multipliers during training. Remembering that margin is $\\frac{2}{||w||_2}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: SVM classification using real dataset\n",
    "\n",
    "1. Expand the following code by adding SVC classification. \n",
    "2. Calculate training accuracy of classification.\n",
    "3. Choose approx C value (in range 0.0001-1.0) in SVC classifer that maximises the accuracy.\n",
    "4. Explain why different C affects classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# get training dataset\n",
    "train = \"data1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:500].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:500].iloc[:, 0].replace(0, -1).to_numpy()\n",
    "\n",
    "X_test = df[500:700].iloc[:, 1:].to_numpy()\n",
    "Y_test = df[500:700].iloc[:, 0].replace(0, -1).to_numpy()\n",
    "\n",
    "# Create an SVM classifier with a linear kernel and fit the model\n",
    "\n",
    "# obtain predictions for the test set\n",
    "\n",
    "#calculate the accuracy\n",
    "\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Optimisation with CVXPY\n",
    "\n",
    "In SVMs, optimization is the process of determining the best parameters for a hyperplane that effectively separates different classes in a dataset. This involves minimizing a cost function to find the optimal weights and bias, with the objective of maximizing the margin between classes while satisfying certain constraints. Concurrently, kernels in SVMs allow these models to handle non-linear data by mapping it into a higher-dimensional space. Kernels, like the radial basis function (RBF) or polynomial kernels, play a pivotal role in shaping the decision boundary. The interplay between optimization and kernels is crucial—the choice of kernel impacts the optimization process, influencing the model's ability to capture intricate patterns in the data and determine the effectiveness of the SVM in handling diverse datasets. Efficient optimization methods become essential, particularly when incorporating kernels to navigate higher-dimensional feature spaces.\n",
    "\n",
    "\n",
    "#### CVXPY example: fitting linear regression\n",
    "\n",
    "In this example we fit linear regression, but put some constraints on W\n",
    "\n",
    " $$J( {\\textbf{W}})=||\\textbf{X}^T \\textbf{W} -  \\textbf{Y}||_2^2$$\n",
    " s. t. \n",
    " $$0 \\le W \\le 1$$\n",
    "\n",
    "\n",
    "Familiarise with the following example, payomg attention to a few important points:\n",
    "* Declaration of variables in CVXPY\n",
    "* Using various atomic functions to manipulate CVXPY variables\n",
    "* The use of a threshold to ensure CVXPY outputs proper values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "zero_threshold = 0.0001\n",
    "\n",
    "# Problem data.\n",
    "m = 30; n = 20\n",
    "np.random.seed(1)\n",
    "X = np.random.randn(m, n)\n",
    "b = np.random.rand()\n",
    "Y = np.random.randn(m)\n",
    "# Construct the problem.\n",
    "W = cp.Variable(n)\n",
    "objective = cp.Minimize(cp.sum_squares((X @ W + b) - Y))\n",
    "constraints = [0 <= W, W <= 1]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "\n",
    "# The optimal value for w is stored in `W.value`.\n",
    "print(W.value)\n",
    "\n",
    "W.value[np.isclose(W.value, 0, atol=zero_threshold)] = 0 \n",
    "print(W.value)\n",
    "\n",
    "# the minimised cost is in result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this code to a larger dataset using the below template. \n",
    "\n",
    "Once the code is working, instead of the expression `cp.sum_squares((X @ W + b) - Y)`, \n",
    "use other cp atomic functions, such as cp.sum, cp.square, cp.norm, cp.matmul. \n",
    "\n",
    "Refer to API reference which can be found here: https://www.cvxpy.org/index.html\n",
    "\n",
    "Then compare the results with the original form. These function will be useful in your Assignment 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# get training dataset\n",
    "train = \"data.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X = df[:2000].iloc[:, 1:].to_numpy()\n",
    "y = df[:2000].iloc[:, 0].replace(0, -1).to_numpy()\n",
    "\n",
    "b = cp.Variable()\n",
    "\n",
    "# Construct the problem.\n",
    "\n",
    "# Get the optimal objective value\n",
    "\n",
    "\n",
    "# Get the optimal W and b\n",
    "\n",
    "print(np.sum(W_val), result)\n",
    "\n",
    "# apply zero threshold\n",
    "zero_threshold = 0.0001\n",
    "\n",
    "# the minimised cost is in result\n",
    "print(np.sum(W_val), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: CSV with kernel\n",
    "\n",
    "In this task, you will compare SVM performance with and without kernel.\n",
    "\n",
    "1. Use the code template to implement the model with and without RBF kernel.\n",
    "2. Run the model and observe the decision boundary for each case. Explain differences.\n",
    "3. Change C values around its current value C=1. Run the model, observe how decision boundary changes with C parameter. Explain these differences.\n",
    "4. Observe how the accuracy changes with and without the kernel, and with differnt values of C. Explain these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = datasets.make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "c = 0.1\n",
    "\n",
    "# Create and fit SVM model with no kernel: svm_model_no_kernel (linear kernel)\n",
    "\n",
    "# Create and fit SVM model with RBF kernel: svm_model_rbf_kernel\n",
    "\n",
    "\n",
    "# Function to plot decision boundary\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary on training data for no kernel\n",
    "plot_decision_boundary(svm_model_no_kernel, X_train, y_train, 'SVM Decision Boundary (No Kernel)')\n",
    "\n",
    "# Plot decision boundary on training data for RBF kernel\n",
    "plot_decision_boundary(svm_model_rbf_kernel, X_train, y_train, 'SVM Decision Boundary (RBF Kernel)')\n",
    "\n",
    "# Evaluate models on test data\n",
    "accuracy_no_kernel = svm_model_no_kernel.score(X_test, y_test)\n",
    "accuracy_rbf_kernel = svm_model_rbf_kernel.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy on test data (No Kernel): {accuracy_no_kernel * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data (RBF Kernel): {accuracy_rbf_kernel * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
