{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50036eab",
   "metadata": {},
   "source": [
    "# Assignment 2 Workbook\n",
    "\n",
    "\n",
    "Please read and sign the **Academic Integrity Declaration** in myUni Assignment 2 by putting you name and ID. Your assignment may receive 0 mark if this declaration is not signed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1ead5",
   "metadata": {},
   "source": [
    "## Question 1a: SVM Primal Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its primal form, that is,\n",
    "\n",
    "$$\\min_{\\mathbf{w},b,\\{\\xi_i\\}} \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C\\sum_{i=1}^N \\xi_i \\nonumber \\\\ s.t.~~ y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i, ~~\\forall i \\nonumber \\\\ \\xi_i \\ge 0 \\nonumber$$\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code **in the indicated space. Do not add or change any other code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14fe37d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp 1.5.2\n",
      "np 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "# !pip install cvxpy --upgrade # if needed\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"cp\", cp.__version__)\n",
    "print(\"np\",np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4d7723",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:1500].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:1500].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9af991c",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:500, 1:].to_numpy()\n",
    "Y_test = df.iloc[:500, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a355188",
   "metadata": {
    "id": "QhiZQBI0Fhy3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of W, b and slack:\n",
      "5861.26\n"
     ]
    }
   ],
   "source": [
    "# train linear svm in primal form\n",
    "def svm_train_primal(data_train, label_train, C):\n",
    "    X, Y = data_train, label_train\n",
    "    n_samples, m_features = np.shape(X)\n",
    "    \n",
    "    W_value = 0\n",
    "    b_value = 0\n",
    "    slack_var_value = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    w = cp.Variable(m_features)\n",
    "    b = cp.Variable()\n",
    "    slack_var = cp.Variable(n_samples)\n",
    "    \n",
    "    objective = cp.Minimize(0.5 * cp.sum_squares(W_value) + C * cp.sum(slack_var_value))\n",
    "    constraints = [cp.multiply(Y, X @ w + b) >= 1 - slack_var, slack_var >= 0]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "\n",
    "    W_value = w.value\n",
    "    b_value = b.value\n",
    "    slack_var_value = slack_var.value\n",
    "# ================================================================\n",
    "\n",
    "    return [W_value, b_value, slack_var_value]\n",
    "\n",
    "# train primal model\n",
    "C = 1\n",
    "model_primal = svm_train_primal(X_train, Y_train, C)\n",
    "\n",
    "# output svm primal form solutions\n",
    "W = model_primal[0]\n",
    "b = model_primal[1]\n",
    "slack = model_primal[2]\n",
    "\n",
    "print('Sum of W, b and slack:')\n",
    "print(np.round(np.sum(W)+np.sum(slack)+b,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd871c",
   "metadata": {},
   "source": [
    "## Question 1b: SVM Primal Accuracy\n",
    "\n",
    "Please complete and run this code and copy the result into Assignment 2 Question 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "# predict accuracy of svm model on test dataset\n",
    "def svm_predict(data_test, label_test, svm_model):\n",
    "    \n",
    "    acc = 0.0\n",
    "    \n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    W, b = svm_model[0], svm_model[1]\n",
    "    \n",
    "    predictions = np.sign(data_test @ W + b)\n",
    "    acc = np.mean(predictions == label_test)\n",
    "# ==========================================================\n",
    "\n",
    "    return acc\n",
    "\n",
    "# output primal accuracy as real number, not percentage\n",
    "accuracy = svm_predict(X_test, Y_test, model_primal)\n",
    "print('Accuracy:')\n",
    "print(np.round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130676a2",
   "metadata": {},
   "source": [
    "## Question 2a: SVM Dual Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its **dual** form, that is,\n",
    "\n",
    "$$\\max_{\\alpha_i}\\sum_i \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j <\\mathbf{x}_i, \\mathbf{x}_j> \\nonumber \\\\ s.t. ~~~ 0 \\le \\alpha_i \\le C\\nonumber \\\\ ~~~ \\sum_i \\alpha_i y_i = 0 \\nonumber$$\n",
    "\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code in the indicated space. **Do not modify any other code**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944c723a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(cp.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9760d985",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:1500].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:1500].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8287b42",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:500, 1:].to_numpy()\n",
    "Y_test = df.iloc[:500, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bee5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of alphas:\n",
      "443.7\n"
     ]
    }
   ],
   "source": [
    "# train linear svm in dual form\n",
    "def svm_train_dual(data_train, label_train, C):\n",
    "    \n",
    "    alphas = 0\n",
    "    x, y = data_train, label_train\n",
    "    n_samples, n_features = np.shape(x)\n",
    "    \n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "    alpha = cp.Variable(n_samples)\n",
    "    K = x @ x.T\n",
    "    # PSD wrap to ensure positive semi-definiteness\n",
    "    Q = cp.psd_wrap((y[:, None] * y[None, :]) * K)\n",
    "\n",
    "    objective = cp.Maximize(cp.sum(alpha) - 0.5 * cp.quad_form(alpha, Q))\n",
    "    constraints = [alpha >= 0, alpha <= C, cp.sum(cp.multiply(alpha, y)) == 0]\n",
    "    \n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve(solver=cp.SCS)\n",
    "    alphas = alpha.value\n",
    "# ===========================================================\n",
    "\n",
    "    # return svm dual model alphas\n",
    "    return alphas\n",
    "\n",
    "# train dual model\n",
    "c = 1\n",
    "alphas = svm_train_dual(X_train, Y_train, c)\n",
    "\n",
    "# output svm dual form solutions\n",
    "print('Sum of alphas:')\n",
    "print(np.round(np.sum(alphas),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3fe63",
   "metadata": {},
   "source": [
    "## Question 2b: Dual model parameters\n",
    "\n",
    "Complete the code where indicated, run it and copy the result into Assignment 2 Question 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e20c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of W and b:\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "# obtain primal w*, b* from dual solution\n",
    "def find_model_params_from_dual(data, label, alphas, C):\n",
    "    \n",
    "    # this value is used to compare values generated by CVXPYY to zero.\n",
    "    zero_threshold = 0.0001\n",
    "    n_samples, n_features = np.shape(data)\n",
    "    a = alphas\n",
    "   \n",
    "    a[np.isclose(a, 0, atol=zero_threshold)] = 0  # zero out nearly zeros\n",
    "    a[np.isclose(a, C, atol=zero_threshold)] = C  # round the ones that are nearly C\n",
    "\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# Note: b_dual is a scalar, but maybe different for each a parameter, therefore calculate the mean\n",
    "    w_dual = np.zeros(n_features)\n",
    "    for i in range(n_samples):\n",
    "        w_dual += a[i] * label[i] * data[i]\n",
    "        \n",
    "    flag = (a > 0) & (a < C)\n",
    "    b = label[flag] - data[flag] @ w_dual\n",
    "    b_dual = np.mean(b)\n",
    "\n",
    "# =========================================================================\n",
    "    return w_dual, b_dual\n",
    "\n",
    "# output reconstructed w* and b* from svm dual problem\n",
    "C = 1\n",
    "model_dual = find_model_params_from_dual(X_train, Y_train, alphas, C)\n",
    "\n",
    "print('Sum of W and b:')\n",
    "print(np.round(np.sum(model_dual[0])+model_dual[1],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2760a",
   "metadata": {},
   "source": [
    "## Question 3: Choosing SVM kernel and soft margin parameter C\n",
    "\n",
    "In this question, you will apply training/testing methodology to choose two parameters for `sklearn.SVC`, which is Python library implementing classification SVM. This trainint/testing methodology is typical for Machine Learning. Since this could be the first time for many students applying training/testing methodology, all steps are specified.\n",
    "\n",
    "Read SVC documentation and examples on how to train and test SVC models. (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "Please use the following steps **strictly** in order to apply correct methodology and obtain correct final result.\n",
    "\n",
    "1. Read the training and test data (already coded in the below template, do not change)\n",
    "2. Split training data into train and validation (dev) sets (already coded in the below template, do not change)\n",
    "\n",
    "Your code will start from this point.\n",
    "\n",
    "3. Create four new svc models and train using x_train with the following parameters: `kernel=k, random_state=42`, where k in ('linear', 'poly', rbf', 'sigmoid'). **Leave the rest of options in the model unchanged**. Test using x_dev, get F1 score for each model. \n",
    "4. Choose the kernel which has highest F1 score, call it best_kernel.\n",
    "5. Create five new svc models and train using x_train with the following parameters: `kernel=best_kernel, C=c, random_state=42`, where c in (0.1, 0.25, 0.5, 1.0, 2.0). **Leave the rest of options in the model unchanged**. Test using x_dev, get F1 score for each model.\n",
    "6. Choose C from point 5 with highest F1. Call it best_c\n",
    "7. Create new svc model and train using **x_train_full** with the following parameters: `kernel=best_kernel, C=best_c, random_state=42`. **leave the rest of options in the model unchanged**. Test using **x_test**, get F1 score. \n",
    "8. Enter output of this code into Question 3 in myUni.\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bd226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 and best C\n",
      "1.894\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This is template for your implementation of Question 3. \n",
    "    Do not add any imports\n",
    "    Do not change any provided code\n",
    "    Write your code in indicated space only\n",
    "'''\n",
    "\n",
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC as svc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Read the training and test data (already coded in the below template, do not change)\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "x_train_full = df[:1500].iloc[:, 1:].to_numpy()\n",
    "y_train_full = df[:1500].iloc[:, 0].replace(0, -1).to_numpy()\n",
    "\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "x_test = df.iloc[:500, 1:].to_numpy()\n",
    "y_test = df.iloc[:500, 0].replace(0, -1).to_numpy()\n",
    "\n",
    "# 2. Split training data into train and validation (devset) sets (already coded in the below template, do not change)\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_train_full, y_train_full, test_size=500, random_state=42)\n",
    "\n",
    "best_f1 = 0 \n",
    "best_c = 0\n",
    "\n",
    "#===================== your code is in this space ====================\n",
    "# Find best kernel\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "f1_scores_k = []\n",
    "\n",
    "for k in kernels:\n",
    "    model = svc(kernel=k, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_dev)\n",
    "    f1_scores_k.append(f1_score(y_dev, y_pred))\n",
    "    \n",
    "best_kernel = kernels[np.argmax(f1_scores_k)]\n",
    "\n",
    "# Find best C\n",
    "C_list = [0.1, 0.25, 0.5, 1.0, 2.0]\n",
    "f1_scores_c = []\n",
    "\n",
    "for c in C_list:\n",
    "    model = svc(kernel=best_kernel, C=c, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_dev)\n",
    "    f1_scores_c.append(f1_score(y_dev, y_pred))\n",
    "    \n",
    "best_c = C_list[np.argmax(f1_scores_c)]\n",
    "\n",
    "# Train final model with best kernel and C\n",
    "final_model = svc(kernel=best_kernel, C=best_c, random_state=42)\n",
    "final_model.fit(x_train_full, y_train_full)\n",
    "\n",
    "y_pred_test = final_model.predict(x_test)\n",
    "best_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "#=====================================================================\n",
    "\n",
    "print(\"Best F1 and best C\")\n",
    "print(np.round(best_f1+best_c,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83ea70-ae9e-4baf-8835-6833f177da60",
   "metadata": {},
   "source": [
    "## Question 4a: PCA implementation\n",
    "\n",
    "1. Perform PCA on the dataset to reduce each sample into a 10-dimensional feature vector.\n",
    "2. Print the required result and enter into Assignment 2 Question 3a.\n",
    "\n",
    "=========================================================================\n",
    "- Implementing PCA algorithm.\n",
    "    - Start\n",
    "        - Input: $m$ number of samples as matrix $X$ of $m$ rows and $n$ columns.\n",
    "        - Calculate the mean vector for each column. $$mean = \\frac {1}{m} \\sum \\limits _{i=1} ^{n}X_{ij}$$\n",
    "        - Calculate the centralised matrix $X_C$ and covariance matrix $C$. $$X_C=X-mean$$ $$C = \\frac {1}{m}(X_C)^TX_C$$\n",
    "        - Calculate the eigenvalues and eigenvectors using convariance matrix.\n",
    "        - Select top $k$ principal components - as eigen vector corresponding to top $k$ eigen values. Construct matrix $P$.\n",
    "    - End\n",
    "    \n",
    "- Transforming the the data using the principal components (matrix $P$) obtained using the PCA algorithm. $$Transformed \\: Data = X_C P$$\n",
    "- Calculating the covariance matrix of the transformed data by first centralising it (mean subtracted) and then obtaining the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c40204",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed4563b",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X = df[:1500].iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55475a58-cd32-4c7f-8263-ffad66c91dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_cov_X_transformed:\n",
      "325.08\n"
     ]
    }
   ],
   "source": [
    "# Selecting top 10 Principal components\n",
    "no_of_components = 10\n",
    "\n",
    "covariance_matrix_X = 0\n",
    "covariance_matrix_X_transformed = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_c = X - X_mean\n",
    "\n",
    "covariance_matrix_X = (1/X.shape[0]) * (X_c.T @ X_c)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix_X)\n",
    "indices = np.argsort(eigenvalues)[::-1][:no_of_components]\n",
    "eigenvectors_selected = eigenvectors[:, indices]\n",
    "X_transformed = X_c @ eigenvectors_selected\n",
    "\n",
    "\n",
    "X_transformed_C = X_transformed - np.mean(X_transformed, axis=0)\n",
    "covariance_matrix_X_transformed = (1 / X_transformed.shape[0]) * X_transformed_C.T @ X_transformed_C\n",
    "\n",
    "# ==========================================================================\n",
    "\n",
    "sum_cov_X = np.sum(covariance_matrix_X)\n",
    "sum_cov_X_transformed = np.sum(covariance_matrix_X_transformed)\n",
    "\n",
    "print(\"sum_cov_X_transformed:\")\n",
    "print(np.round(sum_cov_X + sum_cov_X_transformed,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300ef44",
   "metadata": {},
   "source": [
    "## Question 4b: PCA data reconstructon\n",
    "\n",
    "For this question:\n",
    "1. Reconstruct the X dataset from your results in code Question 3a as X_back.\n",
    "1. Centre  X_back and calculate covariance matrix for X_back and enter its sum into Assignment 2 Question 3b.\n",
    "\n",
    "For this part, use the libraries imported in Question 4a, and do not import any moore libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d53eb57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of covariance_matrix_X_back:\n",
      "68.06\n"
     ]
    }
   ],
   "source": [
    "# Do not import any additional libraries for this section\n",
    "# Do not change any code outside of this area marked with =============================\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "X_C_back = X_transformed @ eigenvectors_selected.T\n",
    "X_back = X_C_back + X_mean\n",
    "\n",
    "X_back_C = X_back - np.mean(X_back, axis=0)\n",
    "covariance_matrix_X_back = (1 / X_back.shape[0]) * (X_back_C.T @ X_back_C)\n",
    "# ============================================================\n",
    "print(\"Sum of covariance_matrix_X_back:\")\n",
    "print(np.round(np.sum(covariance_matrix_X_back),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02272d",
   "metadata": {},
   "source": [
    "## Questions 5a: Kernel k-Means derivation\n",
    "\n",
    "In this question you will derive a new mathematical formula for Kernel k=Means and then implement it.\n",
    "\n",
    "The task is specified as follows:\n",
    "1. We have a dataset with M instances and N features in each instance. We can express this dataset as a set of n-dimensional vectors $\\mathbf{X}=\\{\\mathbf{x}_i \\}\\in \\mathbb{R}^{M\\times N}$. Each of these vectors is $\\mathbf{x}_i \\in \\mathbb{R}^N$. \n",
    "\n",
    "2. We have a vector of averages $\\mathbf{\\mu} = \\frac{1}{M}\\sum_i \\mathbf{x}_i$, where i-th value of that vector is an average of i-th feature over the entire $X$ dataset. Therefore $\\mu$ is an \"average instance\" for that dataset and $\\mathbf{\\mu} \\in \\mathbb{R}^N$.\n",
    "\n",
    "3. Let $m_1 = \\frac{1}{M^2}\\sum_i\\sum_j \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2$ which is average of squared pairwise distance between instances. \n",
    "\n",
    "4. Let $m_2 = \\frac{1}{M} \\sum_i\\|\\mathbf{x}_i - \\mathbf{\\mu}\\|_2^2$ which is variance of distances between each $\\mathbf{x}_i$ and mean $\\mu$.\n",
    "\n",
    "Note that $m_1$ and $m_2$ are scalars.\n",
    "\n",
    "5. k-Means with RBF-kernel is given as follows:\n",
    "\n",
    "$$k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2\\sigma^2})$$\n",
    "\n",
    "As you can see $2\\sigma^2$ can be expressed in terms of $m_1$, which gives you\n",
    "\n",
    "$$k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2m_2})$$\n",
    "\n",
    "Your tasks are as follows:\n",
    "1. Derive the relationship between $m_1$ and $m_2$. Let $m_2=f(m_1)$ be the dependency between $m_1$ and $m_2$.\n",
    "\n",
    "Important: **Include all steps and transformations that you use and comment on them by answering the \"why\" question (why this equals that).** Points may be deducted if a transformation is not clearly explained.\n",
    "\n",
    "2. Replace $m_2$ with your derived $f(m_1)$ in the formula of k-Means with RBF-kernel\n",
    "\n",
    "\n",
    "$$k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{f(m_1)})$$\n",
    "\n",
    "\n",
    "You can use derivation space below or use derivation space in myUni\n",
    "\n",
    "===============================================================================\n",
    "###  Derivation of the relationship between $m_1$ and $m_2$\n",
    "To derive $m_2=f(m_1)$, we need to expand $m_1$ and $m_2$ to find the exact form of the function $f$.\n",
    "\n",
    "#### Expand $m_1$:\n",
    "Given that \n",
    "$$\n",
    "m_1 = \\frac{1}{M^2}\\sum_i\\sum_j \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2\n",
    "$$\n",
    "Substituting the definition of the squared Euclidean distance, we have:\n",
    "\\begin{align*}\n",
    "m_1 \n",
    "&= \\frac{1}{M^2}\\sum_i\\sum_j \\left( \\|\\mathbf{x}_i\\|_2^2 - 2{\\mathbf{x}_i}^\\top \\mathbf{x}_j + \\|\\mathbf{x}_j\\|_2^2 \\right) \\\\\n",
    "&= \\frac{1}{M^2}\\left( \\sum_i\\sum_j \\|\\mathbf{x}_i\\|_2^2 - 2\\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j + \\sum_i\\sum_j \\|\\mathbf{x}_j\\|_2^2 \\right) \\\\\n",
    "&= \\frac{1}{M^2}\\left( M\\sum_i \\|\\mathbf{x}_i\\|_2^2 + M\\sum_j \\|\\mathbf{x}_j\\|_2^2 - 2\\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j \\right) \\\\\n",
    "&= \\frac{2}{M}\\sum_i \\|\\mathbf{x}_i\\|_2^2 - \\frac{2}{M^2}\\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j\n",
    "\\end{align*}\n",
    "\n",
    "#### Expand $m_2$:\n",
    "Now, we expand $m_2$ in terms of dot products:\n",
    "\\begin{align*}\n",
    "    m_2 \n",
    "    &= \\frac{1}{M} \\sum_i\\|\\mathbf{x}_i - \\mathbf{\\mu}\\|_2^2 \\\\\n",
    "    &= \\frac{1}{M} \\sum_i \\left( \\|\\mathbf{x}_i\\|_2^2 - 2{\\mathbf{x}_i}^\\top \\mathbf{\\mu} + \\|\\mathbf{\\mu}\\|_2^2 \\right) \\\\\n",
    "    &= \\frac{1}{M} \\left( \\sum_i \\|\\mathbf{x}_i\\|_2^2 - 2\\sum_i {\\mathbf{x}_i}^\\top \\mathbf{\\mu} + M\\|\\mathbf{\\mu}\\|_2^2 \\right) \\\\\n",
    "\\end{align*}\n",
    "Note that $\\mathbf{\\mu} = \\frac{1}{M}\\sum_i \\mathbf{x}_i$, so we can have\n",
    "$$\n",
    "\\sum_i \\mathbf{x}_i^\\top \\mu = \\sum_i \\mathbf{x}_i^\\top \\left( \\frac{1}{M} \\sum_j \\mathbf{x}_j \\right) = \\frac{1}{M} \\sum_i \\sum_j \\mathbf{x}_i^\\top \\mathbf{x}_j\n",
    "$$\n",
    "Substituting this into the expression for $m_2$, we get:\n",
    "$$\n",
    "m_2 = \\frac{1}{M} \\sum_i \\|\\mathbf{x}_i\\|_2^2 - \\frac{2}{M^2} \\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j + \\|\\mathbf{\\mu}\\|_2^2\n",
    "$$\n",
    "Then we can express $\\|\\mathbf{\\mu}\\|_2^2$ as:\n",
    "$$ \n",
    "\\|\\mathbf{\\mu}\\|_2^2 = \\left( \\frac{1}{M} \\sum_i \\mathbf{x}_i \\right)^\\top \\left( \\frac{1}{M} \\sum_j \\mathbf{x}_j \\right) = \\frac{1}{M^2} \\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j\n",
    "$$\n",
    "Substituting this into the expression for $m_2$, we have:\n",
    "\\begin{align*}\n",
    "m_2 &= \\frac{1}{M} \\sum_i \\|\\mathbf{x}_i\\|_2^2 - \\frac{2}{M^2} \\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j + \\frac{1}{M^2} \\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j \\\\\n",
    "&= \\frac{1}{M} \\sum_i \\|\\mathbf{x}_i\\|_2^2 - \\frac{1}{M^2} \\sum_i\\sum_j {\\mathbf{x}_i}^\\top \\mathbf{x}_j\n",
    "\\end{align*}\n",
    "Thus, comparing the two expressions for $m_1$ and $m_2$, we can see that:\n",
    "$$m_2 = f(m_1) = \\frac{1}{2} m_1$$\n",
    "Which means that the derivation of the relationship between $m_1$ and $m_2$ is $\\frac{1}{2}$.\n",
    "\n",
    "### Replace $m_2$ with your derived $f(m_1)$\n",
    "Given that \n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp \\left( \\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2\\sigma^2} \\right)\n",
    "= \\exp \\left( \\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2m_2} \\right)\n",
    "$$\n",
    "Substituting $m_2 = f(m_1) = \\frac{1}{2} m_1$, we have:\n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp \\left( \\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2 \\cdot \\frac{1}{2} m_1} \\right) = \\exp \\left( \\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{m_1} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a331268",
   "metadata": {},
   "source": [
    "## Question 5b: Kernel k-Means implemention\n",
    "\n",
    "\n",
    "In Question 5a you have proved the relationship between $m_1$ and $m_2$ as $m_2=f(m_1)$\n",
    "\n",
    "In this question you will implement kernel k-means with modified RBF-kernel, using $m_2=f(m_1)$, and using SpectralClustering from sklearn to reduce the programming efford of the full implementation. \n",
    "RBF-kernel to implement is as follows:\n",
    "\n",
    "$$k(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{f(m_1)})$$\n",
    "\n",
    "\n",
    "In order to calculate paiwise distance matrix, please use `scipy.spatial` distance function `cdist`, which is alread pre-loaded in the code. <br>\n",
    "Please check the documentation of the SpectralClustering function to use precomputed affinity matrix, which you are going to supply with the above specification.\n",
    "\n",
    "\n",
    "**NOTE:** to calculate distance, use `distance.cdist` with `\"sqeuclidean\"`\n",
    "\n",
    "\n",
    "For reference, please write **below** the full formula of your RBF kernel replaceing $f(m_1)$ with your derivation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "309b239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df = pd.read_csv('train1.csv', header = None)\n",
    "X = df.iloc[:1000,1:].to_numpy(copy=True)\n",
    "Y = df.iloc[:1000,:1].to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c1d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of centroids:\n",
      "23.95\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "centroids_rbf = np.array(n_clusters)\n",
    "\n",
    "centroids_rbf = []\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\"\"\"\n",
    "# use random_state=0 in the SpectralClustering function to make the results reproducible.\n",
    "\"\"\"\n",
    "dist_sq = distance.cdist(X, X, metric=\"sqeuclidean\")\n",
    "M = X.shape[0]\n",
    "\n",
    "m1 = np.sum(dist_sq) / (M * M)\n",
    "K = np.exp(-dist_sq / m1).astype(float)\n",
    "\n",
    "model = SpectralClustering(\n",
    "    n_clusters=n_clusters,\n",
    "    affinity='precomputed',\n",
    "    random_state=0\n",
    ")\n",
    "labels = model.fit_predict(K)\n",
    "\n",
    "centroids_rbf = np.vstack([X[labels==c].mean(axis=0) for c in range(n_clusters)])\n",
    "# =================================================================\n",
    "    \n",
    "print('Sum of centroids:')\n",
    "print(np.round(np.sum(centroids_rbf),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caf2e7",
   "metadata": {},
   "source": [
    "## Question 6a: Adaboost questions\n",
    "\n",
    "Answer questions in myUni assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b016ae9",
   "metadata": {},
   "source": [
    "## Question 6b: Adaboost code correction\n",
    "The following code shows an incomplete implementation of Adaboost algorithm. The code does not implement the full logic specific to Adaboost. Please modify it to make a correct implementation. After completing the implementation, run the code and copy the result into space provided in myUni Question 6.\n",
    "\n",
    "1. **Do not delete anything, just add what is required to eslack_varsting code. Adding code is sufficient to make correct implementation** <br>\n",
    "2. **You can add code only in sections as indicated.** <br>\n",
    "3. **Please clearly comment the purpose of the code that you added.**\n",
    "4. **Do not change or add any imports**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e6dc33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import exp, log as ln\n",
    "\n",
    "# DO NOT use any other import statements for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94abf099",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df.iloc[:, 1:].to_numpy()\n",
    "Y_train = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44d8ddc5",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:, 1:].to_numpy()\n",
    "Y_test = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "055c51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights for training data\n",
    "def weak_classifier_train(train_data, train_label, sample_weights=None):\n",
    "    # Create a decision stump\n",
    "    stump = DecisionTreeClassifier(max_depth=1)\n",
    "    \n",
    "    # Train the stump using the weighted samples\n",
    "    stump.fit(train_data, train_label, sample_weight=sample_weights) # Added: support for sample_weight\n",
    "    \n",
    "    return stump\n",
    "\n",
    "def weak_classifier_predict(test_data, model):\n",
    "    \n",
    "    return model.predict([test_data]) \n",
    "\n",
    "\n",
    "def Adaboost_train(train_data, train_label, T):\n",
    "\n",
    "# train_data: N x d matrix\n",
    "# train_label: N x 1 vector\n",
    "# T: the number of weak classifiers in the ensemble\n",
    "    # Added: initialize sample weights\n",
    "    N = train_data.shape[0]\n",
    "    sample_weights = np.ones(N) / N  \n",
    "    \n",
    "    ensemble_models = []\n",
    "    alphas = []  # store alpha values\n",
    "    for t in range(0,T):\n",
    "        # model_param_t returns the model parameters of the learned weak classifier\n",
    "        model = weak_classifier_train(train_data, train_label, sample_weights) # Added: pass sample weights\n",
    "        # Added: predict using the weak classifier\n",
    "        pred = model.predict(train_data)\n",
    "        # Added: calculate error\n",
    "        err = np.sum(sample_weights * (pred != train_label)) / np.sum(sample_weights)\n",
    "        # Added: avoid division by zero\n",
    "        err = max(err, 1e-10)\n",
    "        # Added: calculate alpha\n",
    "        alpha = 0.5 * np.log((1 - err) / err)\n",
    "        alphas.append(alpha)\n",
    "        # Added: update sample weights\n",
    "        sample_weights *= np.exp(-alpha * train_label * pred)\n",
    "        sample_weights /= np.sum(sample_weights)  # normalize\n",
    "        # Added: store model and alpha\n",
    "        model = (model, alpha)\n",
    "        ensemble_models.append(model)\n",
    "    \n",
    "    return ensemble_models\n",
    "\n",
    "\n",
    "def Adaboost_test(test_data, ensemble_models):\n",
    "# test_data: n x d\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        decision_ensemble = 0\n",
    "        \n",
    "        for k in range(len(ensemble_models)):\n",
    "\n",
    "            prediction = weak_classifier_predict(test_data[i], ensemble_models[k][0])   # Added: tuple index for model\n",
    "            decision_ensemble = decision_ensemble + prediction * ensemble_models[k][1]  # Added: tuple index for alpha\n",
    "            \n",
    "            if decision_ensemble > 0:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = -1\n",
    "        predictions.append(prediction)\n",
    "            \n",
    "    return predictions\n",
    "\n",
    "# predict and output accuracy\n",
    "\n",
    "ensemble_models = Adaboost_train(X_train, Y_train, 3)\n",
    "predicted_labels = Adaboost_test(X_test, ensemble_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7524ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.876\n"
     ]
    }
   ],
   "source": [
    "######## do not change this code ############\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "print('Accuracy:')\n",
    "print(np.round(accuracy,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
