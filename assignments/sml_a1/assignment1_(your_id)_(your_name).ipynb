{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14637119",
   "metadata": {},
   "source": [
    "# SML Tri1 Assignment 1 Workbook\n",
    "\n",
    "This is the code template for your Assignment 1. You need to add or modify code **only** in the space indicated for each question to get correct results. \n",
    "\n",
    "#### Do not modify any other code. Do not use any other libraries apart from those already in the code\n",
    "\n",
    "### Academic Integrity rules\n",
    "\n",
    "1. You may use external resources, including Gen AI to study and understand concepts.\n",
    "2. You **cannot** use external resources, including Gen AI, to complete any derivation.\n",
    "3. You can use external resources, including Gen AI, to get examples of using required library functions.\n",
    "4. You **cannot** use external resources, including Gen AI, to complete the code, except as in point 3.\n",
    "5. External resources do not include contents of lectures, workshops and myUni pages of this course, which means they can be used in your assignment.\n",
    "6. If you need to use contents of other Uni of Adelaide courses, you must specify which course/page/resource is used.\n",
    "7. Finally, please keep detail record of resources you used in case you need to show originality of your work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c771af9",
   "metadata": {},
   "source": [
    "## Question 1a: Closed Form of Linear Regression: derivation\n",
    "\n",
    "You tasks for Question 1 of this assignment are as follows:\n",
    "1. Read the specification below and derive a formula for W as $W=f(X,Y)$ using the procedure from Lecture 2. \n",
    "2. Verify matrix alignment as given in Lecture Week 1 and 2, using given below number of features and examples.\n",
    "3. Put your derivation and verification in this Notebook in the space provided or in myUni for Question 1a.\n",
    "4. Implement your derived formula in Question 1b of this Notebook, and tun it to calculate values of **W** for the following data.\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & 3 & 4\\\\\n",
    "1 & 6 & 7\\\\\n",
    "1 & 7 & 8\\\\\n",
    "1 & 8 & 9\\\\\n",
    "1 & 11 & 37\n",
    "\\end{bmatrix}, Y =\n",
    "\\begin{bmatrix}\n",
    "13\\\\\n",
    "8\\\\\n",
    "11\\\\\n",
    "2\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note: The first value $x_0=1$ in the above X matrix is for an intercept (aka bias weight) $w_0$. Also remember to configure these matrices to aligh with given linear equation formula.\n",
    "\n",
    "5. Run your code and copy the results to myUni assignment Question 1b space.\n",
    "\n",
    "Linear regression is given as $\\hat{Y}= {X^T} {W}$, <br>\n",
    "where the training dataset $( {X})$ has $m$ examples and $n$ features (where the first feature, for the intercept weight $w_0$, is 1).</br>\n",
    "The ground truth ${Y}$ has $m$ target values. </br>\n",
    "\n",
    "**Your task is to derive formula for** $ {W}= ? $ </br>\n",
    "\n",
    "**Important: comment and justify every transformation of symbols and every step.**\n",
    "\n",
    "For this derivation, you may need the following rules ($A$ is a matrix of constants, not containing $W$):<br> \n",
    "1. $ \\nabla_w(W^TAW)=2AW$ (if $A$ is symetrical), <br>\n",
    "2. $ A^T = A$, if $A$ evaluates to a scalar.\n",
    "3. $ \\nabla_w(W^TA)=A $\n",
    "\n",
    "\n",
    "======= put your derivation here, or in myUni Question 1a, as latex or image, starting with:  =================== <br>\n",
    "$$\\nabla_W(J( {W}))=\\frac{\\partial}{\\partial W}\\frac{1}{2m}||\\hat{ {Y}} -  {Y}||_2^2=...$$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$W=...$$\n",
    "\n",
    "Matrix alignment verification:\n",
    "\n",
    "...\n",
    "\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e25851",
   "metadata": {},
   "source": [
    "## Question 1b: Closed Form of Linear Regression coding\n",
    "\n",
    "Implement the linear regression in marked space to agree with the close form derivation that you proved in task 1a, run the code and enter the results in myUni Question 1b space. </br>\n",
    "\n",
    "**Do not change provided code.** </br>\n",
    "DO NOT use any other import statements for this question. </br>\n",
    "Keep your code within marked space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27925d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "\n",
    "J = 0.0\n",
    "X = None\n",
    "W = None\n",
    "\n",
    "# ====================== Your implementation ======================  \n",
    "# NOTE: to print correctly, sumW and J must be scalar float\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "\n",
    "#============================================================\n",
    "\n",
    "print('Please copy the folowing result to Question 1b in Assignment 1')\n",
    "print(np.round(np.sum(W) + J,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e00ff6",
   "metadata": {},
   "source": [
    "## Question 2: Linear Regression with Gradient Descent\n",
    "\n",
    "Implement linear regression with gradient descent using the below code template. Put your code in provided spaces to obtain correct results. <br>\n",
    "\n",
    "Use the formula of the Linear Regression cost and gradient descent from Lecture week 2.\n",
    "\n",
    "**Dataset**: The dataset used for this question is based on diabetes dataset posted here: https://data.mendeley.com/datasets/wj9rwkp9c2/1, However, the dataset for this assignmnet is not identical to the original one. Please download proper dataset from the Assignment page in myUni</br>\n",
    "The dataset has m=442 instances, and n=3 attributes will be used in the code (the values of these attributes are features in each instance). </br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3795cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "dataset = pd.read_csv(\"diabetes1.csv\")\n",
    "\n",
    "# selecting features\n",
    "X_train = np.array(dataset[[\"sex\", \"bmi\", \"bp\"]])\n",
    "\n",
    "# normalising numerical features\n",
    "X_train = (X_train-np.min(X_train,axis=0))/(np.max(X_train,axis=0)-np.min(X_train,axis=0))\n",
    "\n",
    "# adding '1' column for the intercept\n",
    "X_train = np.concatenate((np.ones(X_train.shape[0]).reshape(X_train.shape[0],1), X_train), axis=1)\n",
    "\n",
    "# forming target\n",
    "Y_train = np.array([dataset[\"target\"]])\n",
    "Y_train = Y_train.reshape(X_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2207a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def linreg_compute_cost(X, Y, W): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      Y (ndarray (m,1)) : target values\n",
    "      W (ndarray (n,1)) : model parameters  \n",
    "      \n",
    "    Returns:\n",
    "      J (scalar): cost\n",
    "    \"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ============================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def linreg_compute_gradient(X, Y, W): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X : Data,m examples with n features\n",
    "      Y : m target values\n",
    "      W : n model parameters \n",
    "      \n",
    "    Returns:\n",
    "      dJ_dW : The gradient of the cost w.r.t. the parameters W, n values. \n",
    "    \"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def linreg_gradient_descent(X, Y, W_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data, n examples with m features\n",
    "      Y                   : m target values\n",
    "      W_in                : n initial model parameters  \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W                   : final n values of parameters \n",
    "      J (scalar)          : final cost\n",
    "      \"\"\"\n",
    "    \n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "    return W, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b86bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.ones(X_train.shape[1]).reshape(X_train.shape[1],1)\n",
    "\n",
    "# gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.05\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate final W and cost J\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# NOTE: to print correctly, sum(W) and J must be scalar float\n",
    "\n",
    "# run linreg_gradient_descent\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "print(\"Please copy the folowing result into Question 2 in myUni Assignment Question 2\")\n",
    "print(np.round(np.sum(W)+J,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fc6a0",
   "metadata": {},
   "source": [
    "## Question 3: Regularised Logistic Regression\n",
    "\n",
    "Now you will implement regularized logistic regression. Complete the code where indicated. **Do not change given template code or API specification.**\n",
    "\n",
    "Recall from Lecture week 4 that the regularized cost function in logistic regression for m instances and n features is\n",
    "\n",
    "$$ J(W) = \\frac{1}{m} \\left[ -Y \\log\\left(h\\left( X \\right) \\right) - \\left( 1 - Y\\right) \\log \\left( 1 - h\\left( X \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n w_j^2 $$\n",
    "\n",
    "\n",
    "predict $ y = 1$ if $‚Ñé_w (ùëß)‚â•0.5, ùëß= w^‚ä§ùë• \\ge 0 $ <br>\n",
    "predict $y = 0$ if $‚Ñé_w (ùëß)<0.5, ùëß= w^‚ä§ùë•<0 $\n",
    "\n",
    "m is number of instances. Note that you should not regularize the parameters $w_0$. \n",
    "\n",
    "Please use the formulas from Lecture week 4 to complete the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ed8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z).astype(\"float\")\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================  \n",
    "    # DO NOT use any other import statements for this question\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT modify code in this cell\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "data = df[['Survived', 'Sex', 'Age', 'SibSp', 'Pclass']].dropna()\n",
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "data = np.array(data)\n",
    "X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "# normalise all cols\n",
    "for c in range(X.shape[1]):\n",
    "    X[:,c] = (X[:,c] - min(X[:,c]))/(max(X[:,c]) - min(X[:,c]))\n",
    "    \n",
    "# break into train/test\n",
    "split = int(0.8 * data.shape[0])\n",
    "\n",
    "X_train = X[:split]\n",
    "X_test = X[split:]\n",
    "Y_train = Y[:split]\n",
    "Y_test = Y[split:]\n",
    "\n",
    "# Add intercept term to X\n",
    "X_train = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def logreg_costFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with L2 regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of n parameters,\n",
    "        where n is the number of features including intercept.\n",
    "    \n",
    "    X : The data set with shape (m,n). m is the number of examples, and\n",
    "        n is the number of features.\n",
    "    \n",
    "    Y : The vector of data labels of size m.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the L2 regularized cost function. \n",
    "   \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea96064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def logreg_GradFunctionReg(W, X, Y, lambda_):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for logistic regression with regularization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Logistic regression vector of n parameters,\n",
    "        where n is the number of features including intercept.\n",
    "    \n",
    "    X : The data set with shape (m,n). m is the number of examples, and\n",
    "        n is the number of features.\n",
    "    \n",
    "    Y : The vector of data labels of size m.\n",
    "    \n",
    "    lambda_ : float\n",
    "        The regularization parameter. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : A vector of size m which is the gradient of the cost\n",
    "        function with respect to theta, at the current values of theta.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = Y.size  # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    grad = np.zeros(W.shape)\n",
    "\n",
    "    # ===================== YOUR CODE HERE ======================\n",
    "\n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39890595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the signature for this function\n",
    "def logreg_gradient_descent_reg(X, Y, W, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X                   : Data ndarray array, n examples with m features\n",
    "      Y                   : ndarray vector of target n values\n",
    "      W                   : ndarray vector of initial m model parameters \n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      W (ndarray)         : Updated values of parameters \n",
    "      \"\"\"\n",
    "    \n",
    "    # ===================== YOUR CODE HERE ======================\n",
    " \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef227feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_W = np.array([-40]*X_train.shape[1])\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 20000\n",
    "alpha = 0.02\n",
    "lambda_ = 2\n",
    "W = 0\n",
    "J = 0\n",
    "acc = 0\n",
    "\n",
    "\"\"\"\n",
    "Apply functions coded above to calculate:\n",
    "    final W after training,\n",
    "    cost J for training set after training\n",
    "    accuracy for test set\n",
    "Use given datasets and parameters\n",
    "\"\"\"\n",
    "\n",
    "# ===================== YOUR CODE HERE ======================\n",
    "# NOTE: to print correctly, sum(W), J and acc must be scalar floats\n",
    "\n",
    "# run gradient descent with regularisation\n",
    "\n",
    "# ===========================================================\n",
    "\n",
    "print('Please copy the folowing result into Question 3 in myUni Assignment Question 3')\n",
    "print(np.round(np.sum(W)+J+acc, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ac2ec",
   "metadata": {},
   "source": [
    "## Question 4: Machine Learning Matrix Operations\n",
    "\n",
    "In machine learning we often transform expressions from one form to another equivalent form. </br>\n",
    "The expression that you are going to transform can be used to sum up all predicted values of a machine learning model, given weights $w$ of the model and all instances $x$.\n",
    "This question asks you to prove equivalence between given and one or more listed expressions.\n",
    "Then implement them using a simple dataset to verify that they really produce the same result.\n",
    "\n",
    "Therefore your tasks are as follows:\n",
    "1. Prove or disprove that expression Eq1 is equivalent to each of the expressions numbered 1 to 4. \n",
    "    You can do so in any of the following ways:\n",
    "    a) space provided below, \n",
    "    b) in myUni equation writer \n",
    "    c) or on the paper converted to PDF and uploaded to myUni.\n",
    "2. Implement Eq1 and all expressions 1 to 4, and use dataset and weights given below to verify equivalence or non-equivalence.\n",
    "3. Copy the result produced by Eq1 to myUni Question4 numeric space (it should be the same as all other equivalent forms)\n",
    "\n",
    "\n",
    "#### Eq1\n",
    "$\n",
    "\\sum_i \\mathbf{x}_i^\\top \\mathbf{w} \\text{ where } \\mathbf{x}_i \\in \\mathbb{R}^{d \\times 1}, \\, \\mathbf{w} \\in \\mathbb{R}^{d \\times 1}.\n",
    "$\n",
    "\n",
    "Candicate equivalent equations:\n",
    "1. $\\sum_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle$\n",
    "2. $\\left( \\sum_i \\mathbf{x}_i \\right) \\mathbf{w}$\n",
    "3. $\\sum_i \\text{Tr}(\\mathbf{x}_i \\mathbf{w}^\\top)$\n",
    "4. $\\mathbf{w}^T \\sum_i \\mathbf{x}_i, \\quad$\n",
    "\n",
    "\n",
    "===================== You can use this space for your proofs ==============================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "===========================================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407e1a9",
   "metadata": {},
   "source": [
    "**Dataset and weights**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3 & x_4 & x_5\\\\\n",
    "1   & 1   & 1   & 1   & 1\\\\\n",
    "3   & 6   & 7   & 8   & 11\\\\\n",
    "4   & 7   & 8   & 9   & 37\\\\\n",
    "\\end{bmatrix}, w =\n",
    "\\begin{bmatrix}\n",
    "13\\\\\n",
    "8\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
